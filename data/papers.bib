% BERTScore (ICLR version)
@inproceedings{zhang_bertscore_2020,
	title        = {{BERTScore: Evaluating Text Generation with BERT}},
	author       = {Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SkeHuCVFDr}
}
% BLEU
@inproceedings{papineni_bleu_2001,
	title        = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle   = {{BLEU}},
	author       = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year         = 2001,
	booktitle    = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '02},
	publisher    = {Association for Computational Linguistics},
	address      = {Philadelphia, Pennsylvania},
	pages        = 311,
	doi          = {10.3115/1073083.1073135},
	url          = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	urldate      = {2021-05-31},
	language     = {en}
}
% CIDEr (CVPR version)
@inproceedings{vedantam_cider_2015,
	title        = {CIDEr: Consensus-based image description evaluation},
	author       = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
	year         = 2015,
	booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {4566--4575},
	doi          = {10.1109/CVPR.2015.7299087},
	url          = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf}
}
% CLAP-sim
@misc{CLAP2023,
      title={Natural Language Supervision for General-Purpose Audio Representations},
      author={Benjamin Elizalde and Soham Deshmukh and Huaming Wang},
      year={2023},
      eprint={2309.05767},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2309.05767}
}
% FENSE (ICASSP2022 version)
@inproceedings{zhou_can_2022,
	title        = {Can Audio Captions Be Evaluated With Image Caption Metrics?},
	author       = {Zhou, Zelin and Zhang, Zhiling and Xu, Xuenan and Xie, Zeyu and Wu, Mengyue and Zhu, Kenny Q.},
	year         = 2022,
	booktitle    = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	volume       = {},
	number       = {},
	pages        = {981--985},
	doi          = {10.1109/ICASSP43922.2022.9746427},
	url          = {https://ieeexplore.ieee.org/abstract/document/9746427}
}
% MACE
@misc{dixit2024maceleveragingaudioevaluating,
      title={MACE: Leveraging Audio for Evaluating Audio Captioning Systems},
      author={Satvik Dixit and Soham Deshmukh and Bhiksha Raj},
      year={2024},
      eprint={2411.00321},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.00321},
}
% METEOR
@inproceedings{denkowski_meteor_2014,
	title        = {Meteor {Universal}: {Language} {Specific} {Translation} {Evaluation} for {Any} {Target} {Language}},
	shorttitle   = {Meteor {Universal}},
	author       = {Denkowski, Michael and Lavie, Alon},
	year         = 2014,
	booktitle    = {Proceedings of the {Ninth} {Workshop} on {Statistical} {Machine} {Translation}},
	publisher    = {Association for Computational Linguistics},
	address      = {Baltimore, Maryland, USA},
	pages        = {376--380},
	doi          = {10.3115/v1/W14-3348},
	url          = {http://aclweb.org/anthology/W14-3348},
	urldate      = {2021-05-31},
	abstract     = {This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation. Meteor Universal brings language speciﬁc evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to signiﬁcantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14).},
	language     = {en},
	pdf          = {https://aclanthology.org/W14-3348.pdf}
}
% ROUGE
@inproceedings{lin_2004_rouge,
	title        = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
	author       = {Lin, Chin-Yew},
	year         = 2004,
	month        = jul,
	booktitle    = {Text Summarization Branches Out},
	publisher    = {Association for Computational Linguistics},
	address      = {Barcelona, Spain},
	pages        = {74--81},
	url          = {https://aclanthology.org/W04-1013}
}
% SPICE (ECCV version)
@inproceedings{anderson_spice_2016,
	title        = {SPICE: Semantic Propositional Image Caption Evaluation},
	author       = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	year         = 2016,
	booktitle    = {Computer Vision -- ECCV 2016},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {382--398},
	isbn         = {978-3-319-46454-1},
	url          = {https://panderson.me/images/SPICE.pdf},
	editor       = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	abstract     = {There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?}
}
% SPIDEr (ICCV version)
@inproceedings{liu_improved_2017,
	title        = {Improved Image Captioning via Policy Gradient optimization of SPIDEr},
	author       = {Siqi Liu and Zhenhai Zhu and Ning Ye and Sergio Guadarrama and Kevin Murphy},
	year         = 2017,
	booktitle    = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice, Italy, October 22-29, 2017},
	publisher    = {{IEEE} Computer Society},
	pages        = {873--881},
	doi          = {10.1109/ICCV.2017.100},
	url          = {https://doi.org/10.1109/ICCV.2017.100},
	timestamp    = {Thu, 11 May 2023 10:42:19 +0200},
	biburl       = {https://dblp.org/rec/conf/iccv/LiuZYG017.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
% SPIDEr-max
@inproceedings{Labbe2022,
    label = {1},
    key = {1},
	title        = {Is my Automatic Audio Captioning System so Bad? SPIDEr-max: A Metric to Consider Several Caption Candidates},
	author       = {Labb\'{e}, Etienne and Pellegrini, Thomas and Pinquier, Julien},
	year         = 2022,
	month        = {November},
	booktitle    = {Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)},
	address      = {Nancy, France},
	url          = {https://dcase.community/documents/workshop2022/proceedings/DCASE2022Workshop_Labbe_46.pdf},
	abstract     = {Automatic Audio Captioning (AAC) is the task that aims to describe an audio signal using natural language. AAC systems take as input an audio signal and output a free-form text sentence, called a caption. Evaluating such systems is not trivial, since there are many ways to express the same idea. For this reason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr, are used to compare a single automatic caption to one or several captions of reference, produced by a human annotator. Nevertheless, an automatic system can produce several caption candidates, either using some randomness in the sentence generation process, or by considering the various competing hypothesized captions during decoding with beam-search, for instance. If we consider an end-user of an AAC system, presenting several captions instead of a single one seems relevant to provide some diversity, similarly to information retrieval systems. In this work, we explore the possibility to consider several predicted captions in the evaluation process instead of one. For this purpose, we propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the scores of several caption candidates. To advocate for our metric, we report experiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On AudioCaps for example, this system reached a SPIDEr-max value (with 5 candidates) close to the SPIDEr human score of reference.}
}
